{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic program ETL ( load, fix data types, fix duplicity, write and read) for \n",
    "\n",
    "# running all steps you need go menu Cell/RunAll\n",
    "\n",
    "# *\n",
    "\n",
    "# Starting\n",
    "\n",
    "# *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import librarys and initial on the Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for convert data types from according map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dtypes(df, list_col_dtypes):\n",
    "\n",
    "    for k, v in list_col_dtypes.items():\n",
    "\n",
    "        if v in [\"int\", \"bigint\"]:\n",
    "            df[k] = pd.to_numeric(df[k], errors='coerce')\n",
    "            df = df.replace(np.nan, 0, regex=True)\n",
    "            df[k] = df[k].astype(int)\n",
    "        elif v == \"datetime64\":\n",
    "            df[k] = pd.to_datetime(df[k])\n",
    "        elif v == \"str\":\n",
    "            df[k] = df[k].apply(str)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load file CSV that was write in another program ( write_csv.ipynb )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_pandas(path_generated):\n",
    "    df = pd.DataFrame()\n",
    "    df = pd.read_csv(path_generated, index_col=0, header=0, low_memory=False)\n",
    "    df.reset_index(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load mapping that has data types corret, primary key of data frame and column that garantees uniqueless of line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mapping():\n",
    "    \n",
    "    list_col_dtypes = {\"id\": \"int\",\n",
    "                       \"randow\": \"str\",\n",
    "                       \"update_timestamp\": \"datetime64\",\n",
    "                       \"boolean\": \"str\"}\n",
    "    \n",
    "    col_name_last_update = \"update_timestamp\"\n",
    "    \n",
    "    primary_key = [\"id\"]\n",
    "    \n",
    "    return [primary_key, col_name_last_update, list_col_dtypes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for remove duplicity of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_duplicity(df, list_col_dtypes, primary_key, col_name_last_update):\n",
    "    \n",
    "    df = fix_dtypes(df, list_col_dtypes)\n",
    "    df.set_index(primary_key, inplace=True)\n",
    "    df[\"rank_tmp\"] = df.groupby(df.index)[col_name_last_update].rank(method=\"first\", ascending=False)\n",
    "    df = df.loc[(df[\"rank_tmp\"] == 1)]\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop([\"rank_tmp\"], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write file parquet in path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet(df, path):\n",
    "    df.to_parquet(path=path, compression='snappy', use_deprecated_int96_timestamps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file parquet in path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet(path):\n",
    "    files = glob.glob(\"{}*.snappy.parquet\".format(path))\n",
    "    data = [pd.read_parquet(f) for f in files]\n",
    "    df = pd.concat(data,ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process that final code output time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(number_rows):\n",
    "    #Read file in storage\n",
    "    path_csv = \"/home/jovyan/work/artifacts/data/input/users/generated_{}_rows.csv\".format(number_rows)\n",
    "\n",
    "    df = load_csv_pandas(path_csv)\n",
    "\n",
    "    #Load mapping\n",
    "    data = load_mapping()\n",
    "\n",
    "    primary_key = data[0]\n",
    "    col_name_last_update = data[1]\n",
    "    list_col_dtypes = data[2]\n",
    "\n",
    "    #Duplicity in file - Fix\n",
    "    df = fix_duplicity(df, list_col_dtypes, primary_key, col_name_last_update)\n",
    "\n",
    "    #Write parquet\n",
    "    path_parquet = \"/home/jovyan/work/artifacts/data/output/pandas/file_pandas.snappy.parquet\"\n",
    "    write_parquet(df, path_parquet)\n",
    "\n",
    "    #Read parquet\n",
    "    path_parquet = \"/home/jovyan/work/artifacts/data/output/pandas/\"\n",
    "    df = read_parquet(path_parquet)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Call process and you put the number of line that you write file csv before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.085846\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "df = process(number_rows=100)\n",
    "end = datetime.now()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is this script makes a count on the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of line on the DF 100\n",
      "0:00:00.000289\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(\"number of line on the DF {}\".format(df.shape[0]))\n",
    "end = datetime.now()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
